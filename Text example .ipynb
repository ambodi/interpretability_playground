{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import sklearn.metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import shap\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import copy\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "class_names = ['atheism', 'christian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=False, token_pattern=r\"\\b\\w+\\b\")\n",
    "train_vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
    "test_vectors = vectorizer.transform(newsgroups_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg = LogisticRegression(random_state=0, solver='lbfgs')\n",
    "lreg.fit(train_vectors, newsgroups_train.target)\n",
    "\n",
    "nbayes = MultinomialNB()\n",
    "nbayes.fit(train_vectors, newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9205607476635513\n",
      "0.8816964285714286\n"
     ]
    }
   ],
   "source": [
    "pred_lreg = lreg.predict(test_vectors)\n",
    "print(sklearn.metrics.f1_score(newsgroups_test.target, pred_lreg, average='binary'))\n",
    "\n",
    "pred_nbayes = nbayes.predict(test_vectors)\n",
    "print(sklearn.metrics.f1_score(newsgroups_test.target, pred_nbayes, average='binary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_nbayes_params = nbayes.coef_[0]\n",
    "global_lreg_params = lreg.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_params_nbayes = np.abs(global_nbayes_params).argsort()[-10:][::-1]\n",
    "selected_params_nbayes_values = global_nbayes_params[selected_params_nbayes]\n",
    "\n",
    "selected_params_lreg = np.abs(global_lreg_params).argsort()[-10:][::-1]\n",
    "selected_params_lreg_values = global_lreg_params[selected_params_lreg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature_names_nbayes = []\n",
    "selected_feature_names_lreg = []\n",
    "for sel_p_val in selected_params_nbayes:\n",
    "    selected_feature_names_nbayes.append(list(vectorizer.vocabulary_.keys())[list(vectorizer.vocabulary_.values()).index(sel_p_val)])  # Prints george\n",
    "    \n",
    "selected_feature_names_lreg = []\n",
    "for sel_p_val in selected_params_lreg:\n",
    "    selected_feature_names_lreg.append(list(vectorizer.vocabulary_.keys())[list(vectorizer.vocabulary_.values()).index(sel_p_val)])  # Prints george"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Naive Bayes weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 83\n",
    "local_gt = train_vectors[idx].nonzero()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 893,  304,  785,  706,  979,  709,  743,  728, 1027,  566])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(0, train_vectors.shape[0], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_weights_nbayes = np.abs(global_nbayes_params[local_gt]).argsort()[-10:][::-1]\n",
    "local_weights_lreg = np.abs(global_lreg_params[local_gt]).argsort()[-10:][::-1]\n",
    "\n",
    "selected_local_feature_names_nbayes = []\n",
    "selected_local_feature_names_lreg = []\n",
    "\n",
    "for sel_p_val in local_weights_nbayes:\n",
    "    selected_local_feature_names_nbayes.append(list(vectorizer.vocabulary_.keys())[list(vectorizer.vocabulary_.values()).index(sel_p_val)])  # Prints george\n",
    "\n",
    "for sel_p_val in local_weights_lreg:\n",
    "    selected_local_feature_names_lreg.append(list(vectorizer.vocabulary_.keys())[list(vectorizer.vocabulary_.values()).index(sel_p_val)])  # Prints george"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIME weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "from lime import lime_text\n",
    "from sklearn.pipeline import make_pipeline\n",
    "c_lreg = make_pipeline(vectorizer, lreg)\n",
    "c_nbayes = make_pipeline(vectorizer, nbayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "split() requires a non-empty pattern match.\n",
      "split() requires a non-empty pattern match.\n"
     ]
    }
   ],
   "source": [
    "explained_class = 1\n",
    "exp_lreg = explainer.explain_instance(newsgroups_train.data[idx], c_lreg.predict_proba, num_features=train_vectors.shape[1], labels=(explained_class, ))\n",
    "exp_lreg_top_10 = explainer.explain_instance(newsgroups_train.data[idx], c_lreg.predict_proba, num_features=10, labels=(explained_class, ))\n",
    "\n",
    "#print('Document id: %d' % idx)\n",
    "#print('Probability(christian) =', c_lreg.predict_proba([newsgroups_train.data[idx]])[0,1])\n",
    "#print('True class: %s' % class_names[newsgroups_train.target[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "split() requires a non-empty pattern match.\n",
      "split() requires a non-empty pattern match.\n"
     ]
    }
   ],
   "source": [
    "explained_class = 1\n",
    "exp_nbayes = explainer.explain_instance(newsgroups_train.data[idx], c_nbayes.predict_proba, num_features=train_vectors.shape[1], labels=(explained_class, ))\n",
    "exp_nbayes_top_10 = explainer.explain_instance(newsgroups_train.data[idx], c_nbayes.predict_proba, num_features=10, labels=(explained_class, ))\n",
    "\n",
    "#print('Document id: %d' % idx)\n",
    "#print('Probability(christian) =', c_nbayes.predict_proba([newsgroups_train.data[idx]])[0,1])\n",
    "#print('True class: %s' % class_names[newsgroups_train.target[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_lime_nbayes_top_10 = []\n",
    "selected_features_lime_lreg_top_10 = []\n",
    "\n",
    "for e in exp_lreg_top_10.as_list():\n",
    "    selected_features_lime_lreg_top_10.append(vectorizer.vocabulary_[e[0]])\n",
    "\n",
    "for e in exp_nbayes_top_10.as_list():\n",
    "    selected_features_lime_nbayes_top_10.append(vectorizer.vocabulary_[e[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features_lime_nbayes = np.zeros((1, train_vectors.shape[1]))\n",
    "selected_features_lime_lreg = np.zeros((1, train_vectors.shape[1]))\n",
    "\n",
    "for e in exp_lreg.as_list():\n",
    "    selected_features_lime_lreg[:, vectorizer.vocabulary_[e[0]]] = e[1]\n",
    "\n",
    "for e in exp_nbayes.as_list():\n",
    "    selected_features_lime_nbayes[:, vectorizer.vocabulary_[e[0]]] = e[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KernelShap Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_train = np.median(train_vectors.toarray(), axis=0).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b8c84f33764139863c85f8e7cd9ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00aa5b3061aa450280e521b4557acadb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lreg_lambda = lambda x: lreg.predict_proba(x)[:, explained_class]\n",
    "nbayes_lambda = lambda x: nbayes.predict_proba(x)[:, explained_class]\n",
    "\n",
    "shap_explainer_nbayes = shap.KernelExplainer(nbayes_lambda, median_train)\n",
    "shap_values_nbayes = shap_explainer_nbayes.shap_values(train_vectors[idx], nsamples=1000)\n",
    "shap_values_nbayes_top_10 = np.abs(shap_values_nbayes.flatten()).argsort()[-10:][::-1]\n",
    "\n",
    "shap_explainer_lreg = shap.KernelExplainer(lreg_lambda, median_train)\n",
    "shap_values_lreg = shap_explainer_lreg.shap_values(train_vectors[idx], nsamples=1000)\n",
    "shap_values_lreg_top_10 = np.abs(shap_values_lreg.flatten()).argsort()[-10:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.86839026]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(shap_values_lreg, selected_features_lime_lreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "explicand = copy.copy(test_vectors[idx].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_replacement(explicand, x_train, selected_features, model, explained_class):\n",
    "    result = 0\n",
    "    for j in selected_features:\n",
    "        explicand_copy = copy.copy(explicand)\n",
    "        explicand_f_val = explicand_copy[:, j][0]\n",
    "        feature_values = train_vectors[:, j].toarray().flatten()\n",
    "        bin_count, bin_edge = np.histogram(feature_values, bins=4)\n",
    "        \n",
    "        for i in range(0, len(bin_edge) - 1):\n",
    "            if explicand_f_val >= bin_edge[i] and  explicand_f_val < bin_edge[i+1]:\n",
    "                bin_idx = i\n",
    "                bin_avg = np.mean([bin_edge[i], bin_edge[i+1]])\n",
    "\n",
    "        prior = bin_count[bin_idx] / len(feature_values)\n",
    "        explicand_copy[:, j] = bin_avg\n",
    "        \n",
    "        new_pred = model.predict_proba(explicand_copy)[0][explained_class]\n",
    "        \n",
    "        result += new_pred * prior\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pred_lreg = lreg.predict_proba(explicand)[0][explained_class]\n",
    "base_pred_nbayes = nbayes.predict_proba(explicand)[0][explained_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred_method_1_lreg_local = predict_replacement(explicand, train_vectors, local_weights_lreg, lreg, explained_class)\n",
    "new_pred_method_1_nbayes_local = predict_replacement(explicand, train_vectors, local_weights_nbayes, lreg, explained_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred_method_1_lreg_lime = predict_replacement(explicand, train_vectors, selected_features_lime_lreg_top_10, lreg, explained_class)\n",
    "new_pred_method_1_nbayes_lime = predict_replacement(explicand, train_vectors, selected_features_lime_nbayes_top_10, lreg, explained_class)\n",
    "new_pred_method_1_lreg_shap = predict_replacement(explicand, train_vectors, shap_values_lreg_top_10, nbayes, explained_class)\n",
    "new_pred_method_1_nbayes_shap = predict_replacement(explicand, train_vectors, shap_values_nbayes_top_10, nbayes, explained_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_change_output(_explicand, train, selected_features, model, explained_class):\n",
    "    explicand_copy = copy.copy(_explicand)\n",
    "\n",
    "    for i in range(0, len(selected_features)):\n",
    "        explicand_copy[:, selected_features[i]] = np.mean(train[:, selected_features[i]]) \n",
    "\n",
    "    new_pred = model.predict_proba(explicand_copy)[0][explained_class]\n",
    "    \n",
    "    return new_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "explicand_copy = copy.copy(explicand)\n",
    "\n",
    "new_pred_method_2_lreg_lime = calculate_change_output(explicand_copy, train_vectors, selected_features_lime_lreg_top_10, lreg, explained_class)\n",
    "new_pred_method_2_nbayes_lime = calculate_change_output(explicand_copy, train_vectors, selected_features_lime_nbayes_top_10, nbayes, explained_class)\n",
    "new_pred_method_2_lreg_shap = calculate_change_output(explicand_copy, train_vectors, shap_values_lreg_top_10, lreg, explained_class)\n",
    "new_pred_method_2_nbayes_shap = calculate_change_output(explicand_copy, train_vectors, shap_values_nbayes_top_10, nbayes, explained_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pred_method_2_lreg_local = calculate_change_output(explicand_copy, train_vectors, local_weights_lreg, lreg, explained_class)\n",
    "new_pred_method_2_nbayes_local = calculate_change_output(explicand_copy, train_vectors, local_weights_nbayes, nbayes, explained_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "explanations_nbayes = pickle.load( open( \"./comparison/explanations_nbayes.p\", \"rb\" ) )\n",
    "explanations_lreg = pickle.load( open( \"./comparison/explanations_lreg.p\", \"rb\" ) )\n",
    "\n",
    "explanations_nbayes_top_10 = pickle.load( open( \"./comparison/explanations_nbayes_top_10.p\", \"rb\" ) )\n",
    "explanations_lreg_top_10 = pickle.load( open( \"./comparison/explanations_lreg_top_10.p\", \"rb\" ) )\n",
    "\n",
    "evaluation_lreg = pickle.load( open( \"./comparison/evaluation_lreg.p\", \"rb\" ) )\n",
    "evaluation_nbayes = pickle.load( open( \"./comparison/evaluation_nbayes.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{83: {'lime': array([0., 0., 0., ..., 0., 0., 0.]),\n",
       "  'shap': array([0., 0., 0., ..., 0., 0., 0.])}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanations_lreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DL)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
